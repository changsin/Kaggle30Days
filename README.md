# 30 Days of Kaggle

## Day1
Signing up and becoming a contributor by doing the first submission using the Titanic dataset.

## Day 2 - 7: Python
Going over Python syntax. For me, these were just a review.

## Day 8 - 11: Intro to ML
### Day 8:
- [How models work? (Lesson 1)](https://www.kaggle.com/dansbecker/how-models-work)
- Basic data exploration - intro to pandas library (Lesson 2)
### Day 9:
- [Your first ML Model (Lesson 3)](https://www.kaggle.com/dansbecker/your-first-machine-learning-model)
- Validation (Lesson 4)
### Day 10:
- [Underfitting and Overfitting (Lesson 5)](https://www.kaggle.com/dansbecker/underfitting-and-overfitting)
  - Overfitting: capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or
  - Underfitting: failing to capture relevant patterns, again leading to less accurate predictions.
- [Random Forests (Lesson 6)](https://www.kaggle.com/dansbecker/random-forests?utm_medium=email&utm_source=gamma&utm_campaign=thirty-days-of-ml&utm_content=day-10)
  "The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters."
  
### Day 11: ML Competitions

